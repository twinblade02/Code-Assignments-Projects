{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN to predict the next word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import gensim\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4779"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8944"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ball_sents = gutenberg.sents('chesterton-ball.txt')\n",
    "simple_sents = [[word.lower() for word in sent if word not in punctuation] for sent in ball_sents]\n",
    "len(simple_sents)\n",
    "w2v = gensim.models.Word2Vec(ball_sents, size = 100, min_count= 1, window = 6, iter = 100)\n",
    "len(w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'ball', 'and', 'the', 'cross', 'by', 'g', 'k', 'chesterton', '1909'],\n",
       " ['i'],\n",
       " ['a', 'discussion', 'somewhat', 'in', 'the', 'air'],\n",
       " ['the',\n",
       "  'flying',\n",
       "  'ship',\n",
       "  'of',\n",
       "  'professor',\n",
       "  'lucifer',\n",
       "  'sang',\n",
       "  'through',\n",
       "  'the',\n",
       "  'skies',\n",
       "  'like',\n",
       "  'a',\n",
       "  'silver',\n",
       "  'arrow',\n",
       "  'the',\n",
       "  'bleak',\n",
       "  'white',\n",
       "  'steel',\n",
       "  'of',\n",
       "  'it',\n",
       "  'gleaming',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bleak',\n",
       "  'blue',\n",
       "  'emptiness',\n",
       "  'of',\n",
       "  'the',\n",
       "  'evening'],\n",
       " ['that',\n",
       "  'it',\n",
       "  'was',\n",
       "  'far',\n",
       "  'above',\n",
       "  'the',\n",
       "  'earth',\n",
       "  'was',\n",
       "  'no',\n",
       "  'expression',\n",
       "  'for',\n",
       "  'it',\n",
       "  'to',\n",
       "  'the',\n",
       "  'two',\n",
       "  'men',\n",
       "  'in',\n",
       "  'it',\n",
       "  'it',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'be',\n",
       "  'far',\n",
       "  'above',\n",
       "  'the',\n",
       "  'stars'],\n",
       " ['the',\n",
       "  'professor',\n",
       "  'had',\n",
       "  'himself',\n",
       "  'invented',\n",
       "  'the',\n",
       "  'flying',\n",
       "  'machine',\n",
       "  'and',\n",
       "  'had',\n",
       "  'also',\n",
       "  'invented',\n",
       "  'nearly',\n",
       "  'everything',\n",
       "  'in',\n",
       "  'it'],\n",
       " ['every',\n",
       "  'sort',\n",
       "  'of',\n",
       "  'tool',\n",
       "  'or',\n",
       "  'apparatus',\n",
       "  'had',\n",
       "  'in',\n",
       "  'consequence',\n",
       "  'to',\n",
       "  'the',\n",
       "  'full',\n",
       "  'that',\n",
       "  'fantastic',\n",
       "  'and',\n",
       "  'distorted',\n",
       "  'look',\n",
       "  'which',\n",
       "  'belongs',\n",
       "  'to',\n",
       "  'the',\n",
       "  'miracles',\n",
       "  'of',\n",
       "  'science'],\n",
       " ['for',\n",
       "  'the',\n",
       "  'world',\n",
       "  'of',\n",
       "  'science',\n",
       "  'and',\n",
       "  'evolution',\n",
       "  'is',\n",
       "  'far',\n",
       "  'more',\n",
       "  'nameless',\n",
       "  'and',\n",
       "  'elusive',\n",
       "  'and',\n",
       "  'like',\n",
       "  'a',\n",
       "  'dream',\n",
       "  'than',\n",
       "  'the',\n",
       "  'world',\n",
       "  'of',\n",
       "  'poetry',\n",
       "  'and',\n",
       "  'religion',\n",
       "  'since',\n",
       "  'in',\n",
       "  'the',\n",
       "  'latter',\n",
       "  'images',\n",
       "  'and',\n",
       "  'ideas',\n",
       "  'remain',\n",
       "  'themselves',\n",
       "  'eternally',\n",
       "  'while',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'evolution',\n",
       "  'that',\n",
       "  'identities',\n",
       "  'melt',\n",
       "  'into',\n",
       "  'each',\n",
       "  'other',\n",
       "  'as',\n",
       "  'they',\n",
       "  'do',\n",
       "  'in',\n",
       "  'a',\n",
       "  'nightmare'],\n",
       " ['all',\n",
       "  'the',\n",
       "  'tools',\n",
       "  'of',\n",
       "  'professor',\n",
       "  'lucifer',\n",
       "  'were',\n",
       "  'the',\n",
       "  'ancient',\n",
       "  'human',\n",
       "  'tools',\n",
       "  'gone',\n",
       "  'mad',\n",
       "  'grown',\n",
       "  'into',\n",
       "  'unrecognizable',\n",
       "  'shapes',\n",
       "  'forgetful',\n",
       "  'of',\n",
       "  'their',\n",
       "  'origin',\n",
       "  'forgetful',\n",
       "  'of',\n",
       "  'their',\n",
       "  'names'],\n",
       " ['that',\n",
       "  'thing',\n",
       "  'which',\n",
       "  'looked',\n",
       "  'like',\n",
       "  'an',\n",
       "  'enormous',\n",
       "  'key',\n",
       "  'with',\n",
       "  'three',\n",
       "  'wheels',\n",
       "  'was',\n",
       "  'really',\n",
       "  'a',\n",
       "  'patent',\n",
       "  'and',\n",
       "  'very',\n",
       "  'deadly',\n",
       "  'revolver']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape:  (8944, 100)\n",
      "Similar words:\n",
      " skies -> sang (0.73),site (0.71),conventions (0.70),fracas (0.69),Ball (0.69),towers (0.68),crypts (0.66),arrow (0.65)\n",
      " professor -> invented (0.78),problems (0.53),surrendered (0.52),justified (0.51),detected (0.51),jerk (0.50),theories (0.48),Professor (0.48)\n",
      " fantastic -> crouching (0.61),hunters (0.57),Against (0.56),unnecessary (0.56),signals (0.55),peculiarly (0.52),sunset (0.52),outlined (0.52)\n",
      " science -> persecutor (0.59),result (0.58),faith (0.55),theological (0.55),physical (0.54),modern (0.53),idiots (0.53),symbol (0.51)\n",
      " evolution -> identities (0.69),degradedly (0.51),melt (0.48),Puritanism (0.48),diving (0.46),Puritanical (0.46),Highlands (0.46),dominance (0.46)\n",
      "Similarity between for and against: 0.19238615\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = w2v.wv.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print('Embedding shape: ', pretrained_weights.shape)\n",
    "print('Similar words:')\n",
    "for word in ['skies','professor','fantastic','science','evolution']:\n",
    "    most_similar = ','.join('%s (%.2f)' % (similar, dist) \n",
    "                            for similar, dist in w2v.wv.most_similar(word)[:8])\n",
    "    print(' %s -> %s' % (word,most_similar))\n",
    "    \n",
    "print('Similarity between for and against: %s' %w2v.wv.similarity('for','against'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    if (word in w2v.wv.vocab):\n",
    "        return w2v.wv.vocab[word].index\n",
    "    return 0\n",
    "# word2idx throws key error when the cell below is run, fixed with above code\n",
    "\n",
    "def idx2word(idx):\n",
    "    return w2v.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4779, 135)\n",
      "(4779,)\n"
     ]
    }
   ],
   "source": [
    "w_count = lambda sentence: len(sentence)\n",
    "max_sent_len = len(max(simple_sents, key = w_count))\n",
    "train_x = np.zeros([len(simple_sents), max_sent_len], dtype = np.int32)\n",
    "train_y = np.zeros([len(simple_sents)], dtype = np.int32)\n",
    "for i, sentence in enumerate(simple_sents):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        if word in w2v.wv.vocab:\n",
    "            train_x[i,t] = word2idx(word)\n",
    "        train_y[i] = word2idx(sentence[-1])\n",
    "# line 7 throws index error even when word2idx is fixed; narrowing it down with the if statement\n",
    "# will solve it\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4779/4779 [==============================] - 2s 382us/step - loss: 9.0504 - accuracy: 0.0688\n",
      "Text after epoch: 0\n",
      "eternally while... -> eternally while noisy gunpowder pitilessly it childish THE pure rowdies plan Devil\n",
      "science and evolution... -> science and evolution bllcr10a others objected mental parallelogram thirteen hundredth improving dismal fresh\n",
      "deadly revolver... -> deadly revolver Seconds things defying ignore blow cutting full career may half\n",
      "revolver... -> revolver fifty stopped pass overheard detonation hart champagne offensive physiognomy wreaths\n",
      "Epoch 2/10\n",
      "4779/4779 [==============================] - 2s 328us/step - loss: 8.9436 - accuracy: 0.1151\n",
      "Text after epoch: 1\n",
      "eternally while... -> eternally while Also childlike shelly islands wrestling soberest chair blew cloudlets legislation\n",
      "science and evolution... -> science and evolution track systems ocean employment achieved indolent gloves redouble proud schoolmaster\n",
      "deadly revolver... -> deadly revolver START demanded lent unity assume led trust those evident irritation\n",
      "revolver... -> revolver dottiness sided ungrateful vulgar report peaked seems committal balustrade Central\n",
      "Epoch 3/10\n",
      "4779/4779 [==============================] - 2s 326us/step - loss: 8.8109 - accuracy: 0.1151\n",
      "Text after epoch: 2\n",
      "eternally while... -> eternally while OTHER orb preparatory fool spades scenery parodied Rule misfortune blasts\n",
      "science and evolution... -> science and evolution typical mood beauty Forest gambits business riding revolted exception leering\n",
      "deadly revolver... -> deadly revolver normal Twenty while Day unsettling boorish spectators irritably Pierre gave\n",
      "revolver... -> revolver smilingly filled extended thee cordially Scotchman illustrate sword syringa Working\n",
      "Epoch 4/10\n",
      "4779/4779 [==============================] - 2s 338us/step - loss: 8.6469 - accuracy: 0.1151\n",
      "Text after epoch: 3\n",
      "eternally while... -> eternally while trailed ---- M apply caught deception Revolution striding vaulted .)\n",
      "science and evolution... -> science and evolution Empires compromise animals jars palpable frantic somewhat Free shadows Before\n",
      "deadly revolver... -> deadly revolver faintly romance native Portions materials recognitions mustn damnable bllcr10a supercilious\n",
      "revolver... -> revolver obedience related bears guard decide VIII secret cursing condition placed\n",
      "Epoch 5/10\n",
      "4779/4779 [==============================] - 2s 328us/step - loss: 8.4467 - accuracy: 0.1151\n",
      "Text after epoch: 4\n",
      "eternally while... -> eternally while stand imprisonment effective York numbers Employ success Entire copies Atheists\n",
      "science and evolution... -> science and evolution inquiry restraint shift slanderer colleges hearts pretensions generalizations plausible leans\n",
      "deadly revolver... -> deadly revolver complicated horseback Star_ You shiny cloud viewless individual helping funny\n",
      "revolver... -> revolver \", hundreds inflexible reverberated Fifty maniacs exchange crimson digression troubled\n",
      "Epoch 6/10\n",
      "4779/4779 [==============================] - ETA: 0s - loss: 8.2167 - accuracy: 0.11 - 2s 329us/step - loss: 8.2113 - accuracy: 0.1151\n",
      "Text after epoch: 5\n",
      "eternally while... -> eternally while generalizations clamorously obliged burst stopping death thinking meadow detective unanswerable\n",
      "science and evolution... -> science and evolution ladies seek maddening strange martyrdom partiality fellows sacredness SUCH a\n",
      "deadly revolver... -> deadly revolver temple prevented companions mended turns Waterloo pitch applicable staccato adventurers\n",
      "revolver... -> revolver angrily oxen cried trusted numbers brisk clipped PROJECT hunters encouragingly\n",
      "Epoch 7/10\n",
      "4779/4779 [==============================] - 2s 331us/step - loss: 7.9669 - accuracy: 0.1151\n",
      "Text after epoch: 6\n",
      "eternally while... -> eternally while Italy Man bodily atmospheres deprecating Frightened beetle Though Epping CAN\n",
      "science and evolution... -> science and evolution beard desolation orator fascination quarters Small beside vulgar thyself attended\n",
      "deadly revolver... -> deadly revolver stalest calculate obsessions battering dogmas expressed chair four spectators apologized\n",
      "revolver... -> revolver bellowing attend VERSIONS rang reason addition qualification new energies bankrupt\n",
      "Epoch 8/10\n",
      "4779/4779 [==============================] - 2s 335us/step - loss: 7.7779 - accuracy: 0.1151\n",
      "Text after epoch: 7\n",
      "eternally while... -> eternally while worse shaggy less moulted hearts signified fourth ANYTHING wire slow\n",
      "science and evolution... -> science and evolution moulted Galahad outlined omen moments Bruno genteel borders Hanoverian crammed\n",
      "deadly revolver... -> deadly revolver tankard quite push broods seconds MUSEUM seraphim Even processing Make\n",
      "revolver... -> revolver quicker quadrangle mount ancient Quintessence yesterday motor loaded abyss chased\n",
      "Epoch 9/10\n",
      "4779/4779 [==============================] - 2s 332us/step - loss: 7.6615 - accuracy: 0.1151\n",
      "Text after epoch: 8\n",
      "eternally while... -> eternally while legged senseless gambling rent plenty damnable rows reverie justly triumphant\n",
      "science and evolution... -> science and evolution domain allowed nasal Down Refund unutterable bewildered codes pitching fun\n",
      "deadly revolver... -> deadly revolver parts nervously heights creed singly bounding Ark organizations admit Socialist\n",
      "revolver... -> revolver work Perfectly yea pole sweet inhuman passing Point XVIII undergrowth\n",
      "Epoch 10/10\n",
      "4779/4779 [==============================] - 2s 336us/step - loss: 7.5888 - accuracy: 0.1151\n",
      "Text after epoch: 9\n",
      "eternally while... -> eternally while blockhead exasperation urbanity pose scornfully neighbours civil stirred seventeenth imaginable\n",
      "science and evolution... -> science and evolution Jersey judgement trusted connected remembering publication implies philosophies galloping sweep\n",
      "deadly revolver... -> deadly revolver destroy hat clenching trunk recently Thornton buzzing sue bush comfortably\n",
      "revolver... -> revolver echoing bad Valencourt humour awaited cheery Mythology clear expects government\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20ae7133cc8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size,weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=embedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# now to generate samples\n",
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds/np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.5)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('Text after epoch: %d' %epoch)\n",
    "    texts = ['eternally while', 'science and evolution', 'deadly revolver', 'revolver']\n",
    "    for text in texts:\n",
    "        sample = generate_next(text)\n",
    "        print('%s... -> %s' % (text, sample))\n",
    "        \n",
    "model.fit(train_x,train_y, batch_size = 150, epochs = 10, \n",
    "          callbacks = [LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "We're able to predict words that come after a given text, though not very well. The addition of a hidden state layer may help boost our predictions (output). We're not getting a good accuracy and the rate of change in the loss function after a certain epoch has run starts to visibly decrease. We may be encountering the vanishing gradient problem here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
